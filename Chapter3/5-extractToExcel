'''
크롤링 결과를 엑셀로 추출하기
'''
import csv
import requests
from bs4 import BeautifulSoup

def blog_crawling(page = 1):
    url = 'https://search.naver.com/search.naver?query=%ED%99%8D%EB%8C%80%20%EB%A7%9B%EC%A7%91&nso=&where=blog&sm=tab_opt'.format(page)
    response = requests.get(url, timeout=10)
    soup = BeautifulSoup(response.text, 'html.parser')
    data = []
    
    for links in soup.select('ul.lst_total > li.bx'):
        title = links.select_one('a.api_txt_lines.total_tit').get_text(strip=True)
        author = links.select_one('a.sub_txt.sub_name').get_text(strip=True)
        date = links.select_one('span.sub_time.sub_txt').get_text(strip=True)
        
        # 정보를 딕셔너리로 저장
        info = {
            'title': title,
            'author': author,
            'date': date
        }
        
        data.append(info)
        
    for item in data:
        print("제목:", item['title'])
        print("작성자:", item['author'])
        print("날짜:", item['date'])
        print()
        
    return data

blog_crawling()

# 엑셀 추출
def save_data(blog_post):
    keys = blog_post[0].keys()
    with open('blog_crawling.csv', 'w') as file:
        writer = csv.DictWriter(file, keys)
        writer.writeheader()
        writer.writerows(blog_post)

blog_post_list = blog_crawling()
save_data(blog_post_list)

# 여러 페이지 추출
blog_post_list = []
for i in range(1, 100, 10):
    blog_post_list.extend( blog_crawling(page=i) )
    #time.sleep(2)
    
save_data(blog_post_list)